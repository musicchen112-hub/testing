# modules/api_clients.py
import streamlit as st
import requests
import time
from difflib import SequenceMatcher
from serpapi import GoogleSearch
import urllib3
import re

# å°å…¥æ¨™é¡Œæ¸…æ´—å‡½å¼
from .parsers import clean_title

# --- å…¨åŸŸ API è¨­å®š ---
S2_API_URL = "https://api.semanticscholar.org/graph/v1/paper/search"
OPENALEX_API_URL = "https://api.openalex.org/works"

MAX_RETRIES = 2
TIMEOUT = 10

# ========== API Key ç®¡ç† ==========
def get_scopus_key():
    return st.secrets.get("scopus_api_key") or _read_key_file("scopus_key.txt")

def get_serpapi_key():
    return st.secrets.get("serpapi_key") or _read_key_file("serpapi_key.txt")

def _read_key_file(filename):
    try:
        with open(filename, "r") as f:
            return f.read().strip()
    except FileNotFoundError:
        return None

# ========== [æ ¸å¿ƒ] 1. ä½œè€…æ¯”å°é‚è¼¯ (æ–°å¢) ==========
def _check_author_match(query_author, result_authors_list):
    """
    å¯¬é¬†æ¯”å°ä½œè€…å§“æ°
    :param query_author: ä½¿ç”¨è€…è¼¸å…¥çš„ä½œè€…å­—ä¸² (ä¾‹å¦‚ "Smith, J." æˆ– "Li")
    :param result_authors_list: API å›å‚³çš„ä½œè€…åˆ—è¡¨ (List of strings or dicts)
    """
    # å¦‚æœä½¿ç”¨è€…æ²’æä¾›ä½œè€…ï¼Œæˆ–æ˜¯è¼¸å…¥çš„ä½œè€…å­—ä¸²å¤ªçŸ­(å¯èƒ½è§£æå¤±æ•—)ï¼Œå°±è·³éæª¢æŸ¥(è¦–ç‚ºé€šé)
    if not query_author or len(query_author) < 2:
        return True
    
    # æå–æŸ¥è©¢ä½œè€…çš„å§“æ° (å‡è¨­æ ¼å¼ç‚º "Family, Given" æˆ– "Family Given")
    # ç°¡å–®ç­–ç•¥ï¼šå–é€—è™Ÿå‰æˆ–ç©ºæ ¼å‰çš„ç¬¬ä¸€å€‹è©ä½œç‚ºå§“æ°
    q_family = re.split(r'[, ]', query_author.strip())[0].lower().strip()
    
    # å¦‚æœå§“æ°å¤ªçŸ­ (ä¾‹å¦‚ "Li", "Ng")ï¼Œæ¯”å°æ™‚è¦å°å¿ƒï¼Œä½†é€™è£¡å…ˆæ¡å¯¬é¬†ç­–ç•¥
    if not q_family: return True

    # è™•ç† API å›å‚³çš„ä½œè€…åˆ—è¡¨
    formatted_results = []
    for auth in result_authors_list:
        if isinstance(auth, dict):
            # é‡å° Crossref/Scopus å¸¸è¦‹çš„ dict çµæ§‹ {'family': 'Smith', 'given': 'John'}
            family = auth.get('family') or auth.get('surname') or auth.get('ce:surname') or ''
            name = auth.get('name') or auth.get('authname') or '' # Semantic Scholar æœ‰æ™‚æ˜¯ 'name'
            formatted_results.append(str(family).lower())
            formatted_results.append(str(name).lower())
        else:
            # ç´”å­—ä¸²
            formatted_results.append(str(auth).lower())
    
    # æª¢æŸ¥ï¼šåªè¦æŸ¥è©¢çš„å§“æ°å‡ºç¾åœ¨ API çµæœçš„ä»»ä½•ä¸€å€‹ä½œè€…åå­—ä¸­ï¼Œå°±ç®— Pass
    for res_str in formatted_results:
        if q_family in res_str:
            return True
            
    return False

# ========== [æ ¸å¿ƒ] 2. æ¨™é¡Œæ¯”å°é‚è¼¯ (åŒ…å«æ‚¨ä¹‹å‰çš„å¯¬é¬†å„ªåŒ–) ==========
# åœ¨ modules/api_clients.py ä¸­æ‰¾åˆ° _is_match å‡½å¼ä¸¦ä¿®æ”¹

def _is_match(query, result):
    if not query or not result: return False
    c_q = clean_title(query)
    c_r = clean_title(result)
    
    # --- æ–°å¢ï¼šå¼·æ•ˆå»å™ª ---
    # ç§»é™¤å¸¸è¦‹çš„éæ¨™é¡Œå­—çœ¼ï¼Œé¿å…å®ƒå€‘å°è‡´æ¯”å°å¤±æ•—
    def remove_noise(text):
        # ç§»é™¤ 4ä½æ•¸å¹´ä»½ (å¦‚ 2023, 2024)
        text = re.sub(r'\b(19|20)\d{2}\b', '', text)
        # ç§»é™¤ arXiv, bioRxiv, Available, Online ç­‰å­—çœ¼
        text = re.sub(r'\b(arxiv|biorxiv|available|online|access)\b', '', text, flags=re.IGNORECASE)
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        return " ".join(text.split())

    c_q = remove_noise(c_q)
    c_r = remove_noise(c_r)
    # ---------------------

    # 1. é‡å° Query æ˜¯é•·æ®µè½... (ç¶­æŒåŸæ¨£)
    if len(c_q) > len(c_r) * 1.5:
        if c_r in c_q: return True

    # 2. ç›¸ä¼¼åº¦æ¯”å° (ç¶­æŒåŸæ¨£)
    ratio = SequenceMatcher(None, c_q, c_r).ratio()
    if ratio >= 0.8: return True  # å»ºè­°ç¨å¾®èª¿é™åˆ° 0.8 ä»¥å®¹å¿å°‘è¨±å·®ç•°
    
    # 3. é—œéµå­—æ¯”å°
    q_words = set(c_q.split())
    r_words = set(c_r.split())
    stop_words = {'a', 'an', 'the', 'of', 'in', 'for', 'with', 'on', 'at', 'by', 'and', 'from', 'to'} # å¢åŠ ä¸€äº›ä»‹ä¿‚è©
    
    # ... (ä¸­é–“çœç•¥) ...

    # åå‘æª¢æŸ¥ (Query çš„é‡è¦å–®å­—éƒ½åœ¨ Result è£¡)
    missing_important_in_result = [w for w in q_words if w not in stop_words and w not in r_words]
    
    # --- æ–°å¢ï¼šå®¹éŒ¯æ©Ÿåˆ¶ ---
    # å¦‚æœåªå·® 1 å€‹å­—ï¼Œä¸”é‚£å€‹å­—å¾ˆçŸ­æˆ–æ˜¯æ•¸å­—ï¼Œæˆ‘å€‘å°±ç•¶ä½œå®ƒæ˜¯é›œè¨Šï¼Œäºˆä»¥é€šé
    if len(missing_important_in_result) <= 1:
        # å¦‚æœ Query å¾ˆé•·ï¼Œå®¹è¨± 1 å€‹å­—çš„èª¤å·®æ˜¯åˆç†çš„
        if len(q_words) >= 5: 
            return True
    # ---------------------

    if len(missing_important_in_result) == 0:
        if len(c_q) > len(c_r) * 0.3:
            return True

    return False

# --- API å‘¼å«è¼”åŠ© ---
def _call_external_api_with_retry(url: str, params: dict, headers=None):
    if not headers: headers = {'User-Agent': 'ReferenceChecker/1.0'}
    for _ in range(MAX_RETRIES):
        try:
            response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)
            if response.status_code == 200: return response.json(), "OK"
            if response.status_code in [401, 403]: return None, f"Auth Error ({response.status_code})"
        except: pass
    return None, "Error"

# ========== 1. Crossref (å«ä½œè€…æ¯”å°) ==========

def search_crossref_by_doi(doi, target_title=None):
    if not doi: return None, None, "Empty DOI"
    clean_doi = doi.strip(' ,.;)]}>')
    url = f"https://api.crossref.org/works/{clean_doi}"
    try:
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            item = response.json().get("message", {})
            titles = item.get("title", [])
            res_title = titles[0] if titles else ""
            
            if target_title and not _is_match(target_title, res_title):
                return None, None, f"DOI Title Mismatch: {res_title[:40]}..."
                
            return res_title, item.get("URL") or f"https://doi.org/{clean_doi}", "OK"
        return None, None, f"HTTP {response.status_code}"
    except: return None, None, "Conn Error"

def search_crossref_by_text(title, author=None):
    if not title: return None, "Empty Title"
    params = {'query.bibliographic': title, 'rows': 2} # æŠ“å‰2ç­†å¢åŠ æ©Ÿæœƒ
    if author:
        params['query.author'] = author # Crossref æ”¯æ´ç›´æ¥æœä½œè€…
        
    data, status = _call_external_api_with_retry("https://api.crossref.org/works", params)
    
    if status == "OK" and data and data.get('message', {}).get('items'):
        for item in data['message']['items']:
            res_title = item.get('title', [''])[0]
            res_authors = item.get('author', []) # å–å¾—ä½œè€…åˆ—è¡¨
            
            # é›™é‡æª¢æŸ¥ï¼šæ¨™é¡Œè¦å° + ä½œè€…è¦å°
            if _is_match(title, res_title):
                if _check_author_match(author, res_authors):
                    return item.get('URL') or f"https://doi.org/{item.get('DOI')}", "OK"
                else:
                    # å¦‚æœæ¨™é¡Œå°ä½†ä½œè€…ä¸å°ï¼Œç¹¼çºŒæ‰¾ä¸‹ä¸€ç­† (å¯èƒ½å‰›å¥½æ˜¯åŒåæ–‡ç« )
                    continue 
                    
        return None, "Match failed (Title or Author mismatch)"
    return None, status

# ========== 2. Scopus (æ–°å¢ä½œè€…æ¯”å°) ==========

def search_scopus_by_title(title, api_key, author=None):
    """
    æ³¨æ„ï¼šapp.py å‘¼å«æ­¤å‡½å¼æ™‚ï¼Œå»ºè­°æ›´æ–°å‚³å…¥ author åƒæ•¸
    """
    if not api_key: return None, "No API Key"
    url = "https://api.elsevier.com/content/search/scopus"
    headers = {"Accept": "application/json", "X-ELS-APIKey": api_key}
    params = {"query": f'TITLE("{title}")', "count": 1}
    
    data, status = _call_external_api_with_retry(url, params, headers)
    
    if status == "OK" and data:
        entries = data.get('search-results', {}).get('entry', [])
        if not entries or 'error' in entries[0]:
            return None, "(No results found)"
        
        match = entries[0]
        res_title = match.get('dc:title', '')
        
        # Scopus çš„ä½œè€…é€šå¸¸åœ¨ 'dc:creator' (ç¬¬ä¸€ä½œè€…) æˆ–éœ€è¦å¦å¤–è§£æ
        # Search API çš„ç°¡å–®å›æ‡‰é€šå¸¸åªçµ¦ 'dc:creator'
        res_creator = match.get('dc:creator', '')
        
        if _is_match(title, res_title):
            if _check_author_match(author, [res_creator]):
                return match.get('prism:url', 'https://www.scopus.com'), "OK"
            else:
                return None, f"Author Mismatch (Found: {res_creator})"
        else:
            return None, f"Title Mismatch: {res_title[:30]}..."
            
    return None, "Error"

# ========== 3. Google Scholar (ç„¡ä½œè€…æ¬„ä½ï¼Œç¶­æŒåŸæ¨£) ==========


def search_scholar_by_title(title, api_key, author=None, raw_text=None):
    st.write(f"ğŸ” æ­£åœ¨å˜—è©¦æŒ‡ç´‹æœå°‹: {title[:30]}...") # é€™æ˜¯æš«æ™‚çš„æ¸¬è©¦ç¢¼ï¼Œæœƒåœ¨ç•«é¢é¡¯ç¤º
    import re
    from serpapi import GoogleSearch

    # 1. æ§‹å»ºæŒ‡ç´‹æœå°‹å­—ä¸² (Fingerprint Query)
    # å¦‚æœåŸå§‹æ–‡æœ¬ä¸­æœ‰ç ”è¨æœƒåç¨± (å¦‚ ICAIT)ï¼Œä¸€å®šè¦æŠ“é€²ä¾†
    conference = ""
    if raw_text:
        conf_match = re.search(r'(ICAIT|CVPR|nips|arxiv|IEEE|ACM)\s*20\d{2}', raw_text, re.I)
        if conf_match:
            conference = conf_match.group(0)

    # çµ„åˆæœå°‹è©ï¼šæ¨™é¡Œå‰æ®µ + ä½œè€… + ç ”è¨æœƒ
    # å°æ–¼ Ko, K. é€™ç­†ï¼Œæœå°‹è©æœƒè®Šæˆ "RAG for Document Query Automation Ko 2024 ICAIT"
    clean_title = re.sub(r'[^\w\s]', '', title)[:60]
    query = f"{clean_title} {author if author else ''} {conference}".strip()

    search = GoogleSearch({
        "q": query,
        "api_key": api_key,
        "engine": "google_scholar",
        "hl": "en"
    })
    
    results = search.get_dict()
    
    if "organic_results" in results:
        # é€™è£¡å¾ˆé—œéµï¼šæˆ‘å€‘æª¢æŸ¥å‰ä¸‰ç­†ï¼Œè€Œä¸æ˜¯åªæŠ“ç¬¬ä¸€ç­†
        for entry in results["organic_results"][:3]:
            found_title = entry.get("title", "")
            found_link = entry.get("link", "")
            
            # æ¯”å°é—œéµå­—æ˜¯å¦å‡ºç¾åœ¨æœå°‹çµæœæ¨™é¡Œä¸­
            # åªè¦ RAG å’Œ Document åŒæ™‚å‡ºç¾ï¼Œå°±æ¥µå¤§æ©Ÿç‡æ˜¯æ­£ç¢ºçš„
            keywords = ["RAG", "Document", "Automation"]
            if all(k.lower() in found_title.lower() for k in keywords[:2]):
                return found_link, found_title
                
    return None, None

    # ==========================================
    # æ­¥é©Ÿ 0: æ™ºæ…§æ¸…æ´—ä½œè€… (é‡å°æ‚¨æåˆ°çš„æ··åˆç‹€æ³)
    # ==========================================
    valid_search_author = None
    if author:
        # 1. å…ˆæŠŠ (et al), [et al], et al. å…¨éƒ¨æ‹¿æ‰
        cleaned = re.sub(r'(?i)[\(\[]?\bet\.?\s*al\.?[\)\]]?', '', author).strip()
        
        # 2. æ¸…ç†ä¹¾æ·¨å¾Œï¼ŒæŠŠé ­å°¾å¤šé¤˜çš„æ¨™é»ç¬¦è™Ÿ (é€—è™Ÿã€å¥è™Ÿã€åˆ†è™Ÿ) ä¿®å‰ªæ‰
        # é€™æ¨£ "Smith, et al." æœƒè®Šæˆ "Smith" (åŸæœ¬æœƒå‰©ä¸‹ "Smith,")
        cleaned = cleaned.strip(' .,;()[]')
        
        if len(cleaned) > 1:
            valid_search_author = cleaned

    # ==========================================
    # æ­¥é©Ÿ 1: æ¨™é¡Œ + ä½œè€… (æœ€æº–ç¢º)
    # ==========================================
    # ç‹€æ³ A: åŸæœ¬æ˜¯ "Smith et al" -> é€™è£¡æœƒæœ "Title Smith" (æˆåŠŸ!)
    # ç‹€æ³ B: åŸæœ¬æ˜¯ "John Smith"  -> é€™è£¡æœƒæœ "Title John Smith" (æ›´æº–!)
    if valid_search_author:
        link, status = _do_search(f'{title} {valid_search_author}', "match (Title+Author)")
        if link: return link, status

    # ==========================================
    # æ­¥é©Ÿ 2: ç´”æ¨™é¡Œ (å¯¬é¬†è£œæ•‘)
    # ==========================================
    # å¦‚æœä½œè€…è§£æå‡ºä¾†æ˜¯ç©ºçš„ï¼Œæˆ–ç¬¬ä¸€é—œæ²’æ‰¾åˆ°ï¼Œè‡ªå‹•é€€å›é€™è£¡
    link, status = _do_search(title, "match (Title Only)")
    if link: return link, status

    # ==========================================
    # æ­¥é©Ÿ 3: åŸå§‹å…¨æ–‡ (çµ‚æ¥µä¿åº•)
    # ==========================================
    if raw_text and len(raw_text) > 10:
        link, status = _do_search(raw_text, "match (Raw Text Fallback)")
        if link: return link, status

    return None, "No match found after 3 attempts"

def search_scholar_by_ref_text(ref_text, api_key, target_title=None):
    if not api_key: return None, "No API Key"
    params = {"engine": "google_scholar", "q": ref_text, "api_key": api_key, "num": 1}
    try:
        results = GoogleSearch(params).get_dict()
        organic = results.get("organic_results", [])
        if organic:
            res_title = organic[0].get("title", "")
            if target_title and not _is_match(target_title, res_title):
                return None, "Title mismatch in fallback"
            return organic[0].get("link"), "similar"
    except: pass
    return None, "No results"

# ========== 4. Semantic Scholar & OpenAlex (å«ä½œè€…æ¯”å°) ==========

def search_s2_by_title(title, author=None):
    # å¢åŠ è«‹æ±‚ 'authors' æ¬„ä½
    params = {'query': title, 'limit': 1, 'fields': 'title,url,authors'}
    data, status = _call_external_api_with_retry(S2_API_URL, params)
    if status == "OK" and data.get('data'):
        match = data['data'][0]
        res_title = match.get('title')
        res_url = match.get('url')
        res_authors = match.get('authors', []) # S2 å›å‚³ [{'authorId':..., 'name': '...'}]

        if _is_match(title, res_title):
            if _check_author_match(author, res_authors):
                return res_url, "OK"
            return None, "Author mismatch"
            
        return None, "Match failed"
    return None, status

def search_openalex_by_title(title, author=None):
    params = {'search': title, 'per_page': 1}
    data, status = _call_external_api_with_retry(OPENALEX_API_URL, params)
    
    if status == "OK" and data.get('results'):
        match = data['results'][0]
        res_title = match.get('title')
        # OpenAlex ä½œè€…çµæ§‹: 'authorships': [{'author': {'display_name': '...'}}]
        res_authors = []
        for authorship in match.get('authorships', []):
            if 'author' in authorship:
                res_authors.append(authorship['author'].get('display_name', ''))

        if _is_match(title, res_title):
            if _check_author_match(author, res_authors):
                url = match.get('doi') or match.get('id')
                if url: return url, "OK"
                return None, "No Link"
            return None, "Author mismatch"
            
        return None, "Title mismatch"
            
    return None, status if status != "OK" else "No results found"

def check_url_availability(url):
    # é€™è£¡åŠ å…¥æ‚¨æéçš„ï¼šéæ¿¾ç´”é¦–é  (ä¾‹å¦‚ https://www.sans.org)
    if not url or not url.startswith("http"): return False
    
    # ç°¡å–®éæ¿¾ï¼šå¦‚æœè·¯å¾‘åªæœ‰ domainï¼Œæ¥µå¤§æ©Ÿç‡æ˜¯é¦–é è€Œéè«–æ–‡é 
    # é‚è¼¯ï¼šè¨ˆç®— '/' çš„æ•¸é‡ã€‚https://abc.com åªæœ‰ 2 å€‹ '/'ã€‚https://abc.com/paper æœ‰ 3 å€‹ã€‚
    if url.count('/') < 3: 
        return False
        
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    try:
        resp = requests.head(url, timeout=5, allow_redirects=True, verify=False)
        return 200 <= resp.status_code < 400

    except: return False

