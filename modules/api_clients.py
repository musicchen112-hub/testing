# modules/api_clients.py
import streamlit as st
import requests
import time
from difflib import SequenceMatcher
from serpapi import GoogleSearch
import urllib3
import re

# å°å…¥æ¨™é¡Œæ¸…æ´—å‡½å¼
from .parsers import clean_title

# --- å…¨åŸŸ API è¨­å®š ---
S2_API_URL = "https://api.semanticscholar.org/graph/v1/paper/search"
OPENALEX_API_URL = "https://api.openalex.org/works"

MAX_RETRIES = 2
TIMEOUT = 10

# ========== API Key ç®¡ç† ==========
def get_scopus_key():
    return st.secrets.get("scopus_api_key") or _read_key_file("scopus_key.txt")

def get_serpapi_key():
    return st.secrets.get("serpapi_key") or _read_key_file("serpapi_key.txt")

def _read_key_file(filename):
    try:
        with open(filename, "r") as f:
            return f.read().strip()
    except FileNotFoundError:
        return None

# ========== [æ ¸å¿ƒ] 1. ä½œè€…æ¯”å°é‚è¼¯ ==========
def _check_author_match(query_author, result_authors_list):
    if not query_author or len(query_author) < 2:
        return True
    
    # æå–å§“æ°
    q_family = re.split(r'[, ]', query_author.strip())[0].lower().strip()
    if not q_family: return True

    formatted_results = []
    for auth in result_authors_list:
        if isinstance(auth, dict):
            family = auth.get('family') or auth.get('surname') or auth.get('ce:surname') or ''
            name = auth.get('name') or auth.get('authname') or ''
            formatted_results.append(str(family).lower())
            formatted_results.append(str(name).lower())
        else:
            formatted_results.append(str(auth).lower())
    
    for res_str in formatted_results:
        if q_family in res_str:
            return True
            
    return False

# ========== [æ ¸å¿ƒä¿®æ­£] 2. æ¨™é¡Œæ¯”å°é‚è¼¯ (é—œéµå„ªåŒ–å€) ==========

def _is_match(query, result):
    if not query or not result: return False
    
    def get_clean_words(text):
        # 1. ç§»é™¤å­¸è¡“æ¨™ç±¤èˆ‡æ‹¬è™Ÿå…§å®¹ (å¦‚ [PDF], (2022))
        text = re.sub(r'\[.*?\]|\(.*?\)', '', str(text))
        # 2. ç§»é™¤é–‹é ­çš„æ•¸å­—ç·¨è™Ÿ (å¦‚ "15. Wang" -> "Wang")
        text = re.sub(r'^\d+[\.\s]+', '', text.strip())
        # 3. è½‰å°å¯«ï¼Œåªç•™è‹±æ–‡èˆ‡æ•¸å­—å–®å­—
        words = re.findall(r'\b[a-z0-9]{3,}\b', text.lower())
        # 4. ç§»é™¤æ¥µåº¦å¸¸è¦‹çš„å¹²æ“¾å­—
        stops = {'the', 'and', 'for', 'with', 'from', 'using', 'based', 'journal', 'researchgate'}
        return [w for w in words if w not in stops]

    q_words = get_clean_words(query)
    r_words = get_clean_words(result)

    # --- é˜²å‘†ï¼šå¦‚æœæ¸…æ´—å¾Œè®Šç©ºçš„ï¼Œå˜—è©¦æ›´ç°¡å–®çš„æ¸…æ´— ---
    if not q_words:
        q_words = re.findall(r'\w+', query.lower())
    if not r_words:
        r_words = re.findall(r'\w+', result.lower())

    if not q_words or not r_words: return False

    # --- æ ¸å¿ƒé‚è¼¯ï¼šé—œéµå­—äº¤é›†ç™¾åˆ†æ¯” ---
    # åªè¦ query è£¡çš„é‡è¦å–®å­—ï¼Œæœ‰ä¸€å®šæ¯”ä¾‹å‡ºç¾åœ¨çµæœä¸­å³å¯
    q_set = set(q_words)
    r_set = set(r_words)
    
    # è¨ˆç®— query çš„å–®å­—æœ‰å¤šå°‘æ¯”ä¾‹å‡ºç¾åœ¨ result ä¸­
    intersection = q_set.intersection(r_set)
    
    # åˆ¤å®š Aï¼šå¦‚æœ query è£¡æœ‰ 60% ä»¥ä¸Šçš„å–®å­—å°ä¸­äº† (æ•‘å› Ko, K.)
    hit_rate = len(intersection) / len(q_set) if q_set else 0
    
    # åˆ¤å®š Bï¼šå‚³çµ±ç›¸ä¼¼åº¦ (é˜²æ­¢æ¥µçŸ­æ¨™é¡Œèª¤åˆ¤)
    c_q_str = "".join(sorted(list(q_set)))
    c_r_str = "".join(sorted(list(r_set)))
    ratio = SequenceMatcher(None, c_q_str, c_r_str).ratio()

    # --- æœ€çµ‚é–€æª»è¨­å®š ---
    # é€™è£¡çš„é–€æª»æ˜¯é—œéµï¼š0.55 æ˜¯è™•ç†ã€Œé«’æ•¸æ“šã€æœ€å¹³è¡¡çš„æ•¸å­—
    if hit_rate >= 0.55:
        return True
    if ratio > 0.7:
        return True
     # åœ¨ _is_match å‡½æ•¸æœ€å¾Œé¢ return False ä¹‹å‰åŠ å…¥ï¼š
     st.write(f"ğŸ” æ¯”å°å¤±æ•—è©³æƒ… | ç›®æ¨™: {q_words} | æœå°‹çµæœ: {r_words} | å‘½ä¸­ç‡: {hit_rate:.2f}")   
    return False
# --- API å‘¼å«è¼”åŠ© ---
def _call_external_api_with_retry(url: str, params: dict, headers=None):
    if not headers: headers = {'User-Agent': 'ReferenceChecker/1.0'}
    for _ in range(MAX_RETRIES):
        try:
            response = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)
            if response.status_code == 200: return response.json(), "OK"
            if response.status_code in [401, 403]: return None, f"Auth Error ({response.status_code})"
        except: pass
    return None, "Error"

# ========== 1. Crossref ==========

def search_crossref_by_doi(doi, target_title=None):
    if not doi: return None, None, "Empty DOI"
    clean_doi = doi.strip(' ,.;)]}>')
    url = f"https://api.crossref.org/works/{clean_doi}"
    try:
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            item = response.json().get("message", {})
            titles = item.get("title", [])
            res_title = titles[0] if titles else ""
            if target_title and not _is_match(target_title, res_title):
                return None, None, f"DOI Title Mismatch"
            return res_title, item.get("URL") or f"https://doi.org/{clean_doi}", "OK"
        return None, None, f"HTTP {response.status_code}"
    except: return None, None, "Conn Error"

def search_crossref_by_text(title, author=None):
    if not title: return None, "Empty Title"
    params = {'query.bibliographic': title, 'rows': 3} 
    data, status = _call_external_api_with_retry("https://api.crossref.org/works", params)
    
    if status == "OK" and data and data.get('message', {}).get('items'):
        for item in data['message']['items']:
            res_title = item.get('title', [''])[0]
            res_authors = item.get('author', [])
            if _is_match(title, res_title):
                if _check_author_match(author, res_authors):
                    return item.get('URL') or f"https://doi.org/{item.get('DOI')}", "OK"
        return None, "Match failed"
    return None, status

# ========== 2. Scopus ==========

def search_scopus_by_title(title, api_key, author=None):
    if not api_key: return None, "No API Key"
    url = "https://api.elsevier.com/content/search/scopus"
    headers = {"Accept": "application/json", "X-ELS-APIKey": api_key}
    params = {"query": f'TITLE("{title}")', "count": 1}
    data, status = _call_external_api_with_retry(url, params, headers)
    
    if status == "OK" and data:
        entries = data.get('search-results', {}).get('entry', [])
        if not entries or 'error' in entries[0]: return None, "No results"
        match = entries[0]
        res_title = match.get('dc:title', '')
        res_creator = match.get('dc:creator', '')
        if _is_match(title, res_title):
            if _check_author_match(author, [res_creator]):
                return match.get('prism:url', 'https://www.scopus.com'), "OK"
    return None, "Mismatch"

# ========== 3. Google Scholar (ä¿®æ­£é‚è¼¯) ==========

def search_scholar_by_title(title, api_key, author=None, raw_text=None):
    if not api_key: return None, "No API Key"
    
    # å…§éƒ¨çš„æœå°‹å°å·¥å…·ï¼ˆä¿®æ­£ç‰ˆï¼‰
    def _do_search(query_string, match_mode):
        try:
            params = {"engine": "google_scholar", "q": query_string, "api_key": api_key, "num": 5}
            search = GoogleSearch(params)
            results = search.get_dict()
            organic = results.get("organic_results", [])
            
            for res in organic:
                res_title = res.get("title", "")
                res_link = res.get("link", "")
                # é€™è£¡æœ€é—œéµï¼šæŠ“å– Google Scholar å›å‚³çš„ä½œè€…ç‰‡æ®µ
                # æ ¼å¼é€šå¸¸æ˜¯ "Y Wang, J Chen - Neural Computing and..., 2022"
                res_author_info = res.get("publication_info", {}).get("summary", "")
                
                # 1. æª¢æŸ¥æ¨™é¡Œ
                if _is_match(title, res_title):
                    # 2. æª¢æŸ¥ä½œè€… (å¦‚æœæˆ‘å€‘æœ‰æä¾› first_author)
                    if valid_search_author:
                        # åªè¦ä½œè€…å§“æ°æ²’å‡ºç¾åœ¨ Google çš„ä½œè€…æ‘˜è¦ä¸­ï¼Œå°±åˆ¤å®šç‚ºèª¤åˆ¤
                        if valid_search_author.lower() in res_author_info.lower():
                            return res_link, res_title
                        else:
                            continue # é›–ç„¶æ¨™é¡Œåƒï¼Œä½†ä½œè€…ä¸å°ï¼Œçœ‹ä¸‹ä¸€ç­†
                    return res_link, res_title
            return None, None
        except: return None, None

    # æ¸…æ´—ä½œè€…
    valid_search_author = None
    if author:
        cleaned = re.sub(r'(?i)[\(\[]?\bet\.?\s*al\.?[\)\]]?', '', author).strip()
        cleaned = cleaned.strip(' .,;()[]')
        if len(cleaned) > 1: valid_search_author = cleaned

    # æ­¥é©Ÿ 1: æ¨™é¡Œ + ä½œè€…
    if valid_search_author:
        link, status = _do_search(f'{title} {valid_search_author}', "match (Title+Author)")
        if link: return link, status

    # æ­¥é©Ÿ 2: ç´”æ¨™é¡Œ
    link, status = _do_search(title, "match (Title Only)")
    if link: return link, status

    # æ­¥é©Ÿ 3: åŸå§‹å…¨æ–‡ä¿åº• (é‡å° Ko, K. æœ€æœ‰æ•ˆçš„ä¸€æ‹›)
    if raw_text:
        # ç¸®çŸ­å…¨æ–‡é¿å…æœå°‹éè¼‰
        short_raw = raw_text[:150]
        link, status = _do_search(short_raw, "match (Raw Text Fallback)")
        if link: return link, status

    return None, "Not found"

def search_scholar_by_ref_text(ref_text, api_key, target_title=None):
    if not api_key: return None, "No API Key"
    params = {"engine": "google_scholar", "q": ref_text[:150], "api_key": api_key, "num": 1}
    try:
        results = GoogleSearch(params).get_dict()
        organic = results.get("organic_results", [])
        if organic:
            res_title = organic[0].get("title", "")
            if target_title and not _is_match(target_title, res_title):
                return None, "Mismatch"
            return organic[0].get("link"), "similar"
    except: pass
    return None, "No results"

# ========== 4. Semantic Scholar & OpenAlex ==========

def search_s2_by_title(title, author=None):
    params = {'query': title, 'limit': 2, 'fields': 'title,url,authors'}
    data, status = _call_external_api_with_retry(S2_API_URL, params)
    if status == "OK" and data.get('data'):
        for match in data['data']:
            res_title = match.get('title')
            res_authors = match.get('authors', [])
            if _is_match(title, res_title):
                if _check_author_match(author, res_authors):
                    return match.get('url'), "OK"
    return None, status

def search_openalex_by_title(title, author=None):
    params = {'search': title, 'per_page': 2}
    data, status = _call_external_api_with_retry(OPENALEX_API_URL, params)
    if status == "OK" and data.get('results'):
        for match in data['results']:
            res_title = match.get('title')
            res_authors = [a['author'].get('display_name', '') for a in match.get('authorships', []) if 'author' in a]
            if _is_match(title, res_title):
                if _check_author_match(author, res_authors):
                    return match.get('doi') or match.get('id'), "OK"
    return None, status

def check_url_availability(url):
    if not url or not url.startswith("http"): return False
    if url.count('/') < 3: return False
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    try:
        resp = requests.head(url, timeout=5, allow_redirects=True, verify=False)
        return 200 <= resp.status_code < 400
    except: return False





